{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamental NLP Concepts with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy is a NLP package for Python (implemented in Cython)\n",
    "#### Common Tasks include:\n",
    "* Tokenization\n",
    "* Lemmatization - reducing inflectional forms of words into base or root word\n",
    "* Part-of-speech Tagging - marking a word based on POS (definition and context\n",
    "* Entity Recognition - identify and segment Named Entities, and categorize them\n",
    "* Dependency Parsing - analyze structure, relationships between Head words, and modifiers\n",
    "* Sentence Recognition\n",
    "* Word-to-vector Transformations\n",
    "* Methods for Cleaning and Normalizing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load spaCy's Pipeline (stored as nlp) - Invoke NLP on sample text to create a Doc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "doc = nlp(\"The big grey dog ate all of the chocolate, but fortunately he wasn't sick!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "#### Splitting text into words, symbols, punctuation, spaces, etc. - creating Tokens\n",
    "#### Can simply split the string on Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'big',\n",
       " 'grey',\n",
       " 'dog',\n",
       " 'ate',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'chocolate,',\n",
       " 'but',\n",
       " 'fortunately',\n",
       " 'he',\n",
       " \"wasn't\",\n",
       " 'sick!']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This Naive approach fails to separate *wasn't* - spaCy can handle this\n",
    "#### Returns a string representations of the token, rather than a token object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'big',\n",
       " 'grey',\n",
       " 'dog',\n",
       " 'ate',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'chocolate',\n",
       " ',',\n",
       " 'but',\n",
       " 'fortunately',\n",
       " 'he',\n",
       " 'was',\n",
       " \"n't\",\n",
       " 'sick',\n",
       " '!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[token.orth_ for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token methods offer string and integer representations of processed text\n",
    "#### Return Strings - methods with underscore_\n",
    "#### Return Integers - methods without underscore suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(The, 'The', 5059648917813135842),\n",
       " (big, 'big', 15511632813958231649),\n",
       " (grey, 'grey', 10475807793332549289),\n",
       " (dog, 'dog', 7562983679033046312),\n",
       " (ate, 'ate', 10806788082624814911),\n",
       " (all, 'all', 13409319323822384369),\n",
       " (of, 'of', 886050111519832510),\n",
       " (the, 'the', 7425985699627899538),\n",
       " (chocolate, 'chocolate', 10946593968795032542),\n",
       " (,, ',', 2593208677638477497),\n",
       " (but, 'but', 14560795576765492085),\n",
       " (fortunately, 'fortunately', 13851269277375979931),\n",
       " (he, 'he', 1655312771067108281),\n",
       " (was, 'was', 9921686513378912864),\n",
       " (n't, \"n't\", 2043519015752540944),\n",
       " (sick, 'sick', 14841597609857081305),\n",
       " (!, '!', 17494803046312582752)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(token, token.orth_, token.orth) for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "#### Reducing a word to its Base form - desirable approach to Standardizing\n",
    "#### Use .lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['practice', 'practice', 'practicing']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "practice = \"practice practiced practicing\"\n",
    "\n",
    "nlp_practice = nlp(practice)\n",
    "[word.lemma_ for word in nlp_practice]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing this prior to certain tasks (such as Bag-of-words) can avoid duplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "#### Assign grammatical properties to words (such as noun, verb, adjective, etc.) - useful in rule-based processes\n",
    "#### For example, to determine who owns what in a given description - Exploit possessives \n",
    "#### Coarse-grained POS - Use .pos_\n",
    "#### Fine-grained POS - Use .tag_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Conor, 'NNP'),\n",
       " ('s, 'POS'),\n",
       " (dog, 'NN'),\n",
       " ('s, 'POS'),\n",
       " (toy, 'NN'),\n",
       " (was, 'VBD'),\n",
       " (hidden, 'VBN'),\n",
       " (under, 'IN'),\n",
       " (the, 'DT'),\n",
       " (man, 'NN'),\n",
       " ('s, 'POS'),\n",
       " (sofa, 'NN'),\n",
       " (in, 'IN'),\n",
       " (the, 'DT'),\n",
       " (woman, 'NN'),\n",
       " ('s, 'POS'),\n",
       " (house, 'NN')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = nlp(\"Conor's dog's toy was hidden under the man's sofa in the woman's house\")\n",
    "\n",
    "pos_tags = [(i, i.tag_) for i in doc2]\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This can be used to Extract the owner, and what they own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Conor, dog), (dog, toy), (man, sofa), (woman, house)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "owners_possessions = []\n",
    "for i in pos_tags:\n",
    "    if i[1] == \"POS\":\n",
    "        owner = i[0].nbor(-1)\n",
    "        possession = i[0].nbor(1)\n",
    "        owners_possessions.append((owner, possession))\n",
    "\n",
    "owners_possessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This returns a List of Tuples - It can also be done as a List Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Conor, dog), (dog, toy), (man, sofa), (woman, house)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i[0].nbor(-1), i[0].nbor(+1)) for i in pos_tags if i[1] == \"POS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Recognition\n",
    "#### Classify Named Entities found in text into Predefined Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Barack Obama, 'PERSON', 380),\n",
       " (American, 'NORP', 381),\n",
       " (44th, 'ORDINAL', 396),\n",
       " (the United States, 'GPE', 384),\n",
       " (2009, 'DATE', 391),\n",
       " (2017, 'DATE', 391),\n",
       " (first, 'ORDINAL', 396),\n",
       " (African, 'NORP', 381),\n",
       " (first, 'ORDINAL', 396),\n",
       " (United\n",
       "  States, 'ORG', 383)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_obama = \"\"\"Barack Obama is an American politician who served as the 44th \n",
    "President of the United States from 2009 to 2017. He is the first African American \n",
    "to have served as president, as well as the first born outside the contiguous United\n",
    "States\"\"\"\n",
    "\n",
    "nlp_obama = nlp(wiki_obama)\n",
    "[(i, i.label_, i.label) for i in nlp_obama.ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entities have been identified - PERSON, NORP, ORDINAL, DATE\n",
    "#### NLP tasks frequently look to split a document into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence number 1: Barack Obama is an American politician who served as the 44th \n",
      "President of the United States from 2009 to 2017.\n",
      "Sentence number 2: He is the first African American \n",
      "to have served as president, as well as the first born outside the contiguous United\n",
      "States\n"
     ]
    }
   ],
   "source": [
    "for ix, sent in enumerate(nlp_obama.sents, 1):\n",
    "    print(\"Sentence number {}: {}\".format(ix, sent))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
